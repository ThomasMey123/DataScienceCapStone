<style>
.small-code pre code {
  font-size: 1em;
}
</style>



Word Prediction Application
========================================================
author: Thomas Mey
date: 17.04.2016


This slidedeck describes an application that suggests words based on previous input, e.g. if you entered "Let's get" the application would suggest 6 words, e.g. "together". Try out! 
The application can be found [here](https://thomasmey123.shinyapps.io/WordPredictionAppV0/). 


The following slides describe the algorithm, the prediction performance and show the applicaiton.

[//]: # "The goal of this exercise is to pitch your data product to your boss or an investor. The slide deck is constrained to be 5 slides or less and should:" 
[//]: # "(1) explain how your model works, "
[//]: # "(2) describe its predictive performance quantitatively and "
[//]: # "(3) show off the app and how it works. "


How does the model work? - I
========================================================
<small>Simply put the model consists of the following steps: Extracting train and test data, Preprocessing, Building and minimizing NGram-tables, Predicting based on NGrams  

- <b>Training samples</b> with different sizes of the original corpus are extracted for two purposes: Analyze how the prediction quality and prediction performance change with different sets of sample sizes. Test data was set aside before sampling the training data. 
- In the <b>preprocessing</b> (using the tm package) all charcters are removed except a-zA-Z and the single quote (e.g. to treat don't as one word), everything is converted to lowercase and profanitiy terms are removed. Note that the prediciton uses the same preprocessing and only lowercase words will be suggested.
- <b>NGrams</b> of order 1 to 3 are build with quanteda (because i had performance problems with Weka tokenizer and rm package) an NGram table with NGram frequencies is build for order. Based on those Kneser-Ney smoothing was applied to lower order NGram tables (1 to 2) - again only with frequencies instead of proper probabilities. 
</small>

How does the model work? - II
========================================================
<small>
For performance reasons the n-grams have been stored in an optimized way that allows fast access: the first resp. first two words are used as a key for bi/trigrams and the most probable continuations are stored as a blank separated list.

```{r echo=FALSE}
t2<-read.csv("./data/55/ngrams/N2GramsKNSC.csv")
colnames(t2)<-c("key", "continuations")
suppressWarnings( require(htmlTable))
htmlTable(t2[3627:3631,1:2],rnames =FALSE,align=paste(rep("l",ncol(t2), collapse="")),
          align.header=paste(rep("l",ncol(t2), collapse="")))
```

The <b>prediction algorithm</b> uses the NGrams with stupid backoff, i.e. it looks for a trigram that matches the last two words of input, if less than the desired tokens are found it look for a bigram that matches the last word, finally it looks for a unigram.
</small>

Performance of the prediction algorithm
========================================================

2.How can you succinctly quantitatively summarize the performance of your prediction algorithm?
<small>
<b>Performance as in quality</b> or: how well does the model predict the next word? Out of a testset strings have been subsampled to emulate a real world typing experience, <b>TO DO: example</b>.
The model performance should match what the application does, so test strings are preprocessed (including lowercase conversion) and 6 word are predicted. If one of the 6 words matches the probe the test result is true, otherwise false.



Performance as in response time

</small>

The Application
========================================================
The application is very simple, it just uses some text input and allows the user to choose one recommendation. This mimics the experience you would have e.g. on a mobile device.

![alt text](applicationScreenshot.png) 


