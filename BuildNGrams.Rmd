---
title: "DataScienceCapstone"
author: "Thomas Mey"
date: "17.3.2016"
output: html_document
---




## setup environment
```{r}
processing.size<-"small"

directory<-paste0(".\\data\\", processing.size, "\\en_US")
nGramDirectory<-paste0(".\\data\\", processing.size, "\\NGrams")
if(!file.exists(nGramDirectory)) {
     dir.create(nGramDirectory)
}
```

## read profanity
```{r }
con <- file(".\\data\\profanity_en.txt", "r") 
p<-readLines(con) 
close(con)
``` 


## print summary statistics
```{r}
#Define functions for counting lines, character and words
lineCount<-function(x) {length(x$content)}
charCount<-function(x) {sum(nchar(x$content))}
wordCount<-function(x) {
    sum(sapply(gregexpr("([[:alpha:]])*", x$content), function(x) sum(x > 0)))
}
performCount<-function(d,f) {sapply(1:length(d),function(x){f(d[[x]])})}

#Function for summary as a table
docsSummary<-function(docs) {
  #get line, character and wordcounts
  l2<-performCount(docs,lineCount)
  c2<-performCount(docs,charCount)
  w2<-performCount(docs,wordCount)
  
  
  #Print table with line, character and word counts after cleansing
  rnames<-NULL
  for(i in 1:3){ rnames<-c(rnames,docs[[i]]$meta$id)}
  d<-data.frame(l2,w2,c2)
  rownames(d)<-rnames
  colnames(d)<-c("#Lines","#Words","#Characters")
  suppressWarnings( require(htmlTable))
  htmlTable(d)
}
```

#preprocess and build dtm
```{r}
options(mc.cores=1)

source("Common.r")

library(tm)   
docs <- Corpus(DirSource(directory))
docsSummary(docs)

docs<-preprocess(docs,p)

docs[[3]]$content[500]

corpus_df <- data.frame(text=lapply(docs[1], '[',"content"), stringsAsFactors=F)
corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[2], '[',"content"), stringsAsFactors=F))
corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[3], '[',"content"), stringsAsFactors=F))


corpus_df<-corpus_df$content
corpus_df[19]

corpus_df<-paste(corpus_df,collapse = "")

require(quanteda)

ng1<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=1)
ng2<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=2)
ng3<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=3)

require(data.table)
nGramTable <- function( nGramTokens) {
  nGramTable <- data.frame(table(nGramTokens))
  colnames(nGramTable) <- c("word", "freq")
  
  nGramTable <-  nGramTable[order(nGramTable$freq,decreasing=TRUE),]
 
  nGramTable
}


ngt1<-nGramTable(ng1)
rownames(ngt1)<-NULL
ngt1<-ngt1[ngt1$freq>1,]
head(ngt1)
tail(ngt1)
write.csv(ngt1,file= paste0(nGramDirectory,"\\N1Grams.csv"),row.names = FALSE)


ngt2<-nGramTable(ng2)
rownames(ngt2)<-NULL
ngt2<-ngt2[ngt2$freq>1,]
head(ngt2)
tail(ngt2)
write.csv(ngt2,file= paste0(nGramDirectory,"\\N2Grams.csv"),row.names = FALSE)

ngt3<-nGramTable(ng3)
rownames(ngt3)<-NULL
ngt3<-ngt3[ngt3$freq>1,]
head(ngt3)
tail(ngt3)
write.csv(ngt3,file= paste0(nGramDirectory,"\\N3Grams.csv"),row.names = FALSE)
```




