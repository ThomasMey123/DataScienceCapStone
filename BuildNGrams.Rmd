---
title: "DataScienceCapstone"
author: "Thomas Mey"
date: "17.3.2016"
output: html_document
---




## setup environment
```{r}
library(data.table)
library(dplyr)

processing.size<-"small"

directory<-paste0(".\\data\\", processing.size, "\\en_US")
nGramDirectory<-paste0(".\\data\\", processing.size, "\\NGrams")
if(!file.exists(nGramDirectory)) {
     dir.create(nGramDirectory)
}
```

## read profanity
```{r }
con <- file(".\\data\\profanity_en.txt", "r") 
p<-readLines(con) 
close(con)
``` 


## print summary statistics
```{r}
#Define functions for counting lines, character and words
lineCount<-function(x) {length(x$content)}
charCount<-function(x) {sum(nchar(x$content))}
wordCount<-function(x) {
    sum(sapply(gregexpr("([[:alpha:]])*", x$content), function(x) sum(x > 0)))
}
performCount<-function(d,f) {sapply(1:length(d),function(x){f(d[[x]])})}

#Function for summary as a table
docsSummary<-function(docs) {
  #get line, character and wordcounts
  l2<-performCount(docs,lineCount)
  c2<-performCount(docs,charCount)
  w2<-performCount(docs,wordCount)
  
  
  #Print table with line, character and word counts after cleansing
  rnames<-NULL
  for(i in 1:3){ rnames<-c(rnames,docs[[i]]$meta$id)}
  d<-data.frame(l2,w2,c2)
  rownames(d)<-rnames
  colnames(d)<-c("#Lines","#Words","#Characters")
  suppressWarnings( require(htmlTable))
  htmlTable(d)
}
```

#preprocess and build dtm
```{r}
options(mc.cores=1)

source("WordPredictionApp/Common.r")

library(tm)   
docs <- Corpus(DirSource(directory))
docsSummary(docs)

docs<-preprocess(docs,p)

docs[[3]]$content[500]

corpus_df <- data.frame(text=lapply(docs[1], '[',"content"), stringsAsFactors=F)
corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[2], '[',"content"), stringsAsFactors=F))
corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[3], '[',"content"), stringsAsFactors=F))


corpus_df<-corpus_df$content
corpus_df[19]

corpus_df<-paste(corpus_df,collapse = "")

require(quanteda)

ng1<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=1)
ng2<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=2)
ng3<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=3)
#ng4<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=4)


require(data.table)
nGramTable <- function( nGramTokens) {
  nGramTable <- data.frame(table(nGramTokens))
  colnames(nGramTable) <- c("word", "freq")
  
  #nGramTable <-  nGramTable[order(nGramTable$freq,decreasing=TRUE),]
  nGramTable[,1]<-as.character(nGramTable[,1])
  nGramTable
}

# Build NGram Tables
ngt1<-nGramTable(ng1)
rownames(ngt1)<-NULL

write.csv(ngt1,file= paste0(nGramDirectory,"\\N1GramsF.csv"),row.names = FALSE)



ngt2<-nGramTable(ng2)
rownames(ngt2)<-NULL
write.csv(ngt2,file= paste0(nGramDirectory,"\\N2GramsF.csv"),row.names = FALSE)

ngt3<-nGramTable(ng3)
rownames(ngt3)<-NULL
write.csv(ngt3,file= paste0(nGramDirectory,"\\N3GramsF.csv"),row.names = FALSE)

#ngt4<-nGramTable(ng4)
#rownames(ngt4)<-NULL
#ngt41<-ngt3[ngt4$freq>1,]
#head(ngt41)
#tail(ngt41)
#write.csv(ngt41,file= paste0(nGramDirectory,"\\N4Grams.csv"),row.names = FALSE)

rm(corpus_df,ng1,ng2,ng3)
```


```{r}
ngt1<-setorder(ngt1, -freq )
write.csv(ngt1[ngt1$freq>1,],file= paste0(nGramDirectory,"\\N1Grams.csv"),row.names = FALSE)


# prepare ng2 for witing
ngt2<-cbind(ngt2,data.frame(tstrsplit(ngt2[,1],split = " ")))
colnames(ngt2)<-c("word","freq","t1","t2")
#ngt2$t1<-as.character(ngt2$t1)
#ngt2$t2<-as.character(ngt2$t2)
ngt2<-setorder(ngt2, t1, -freq )
ngt2<-select(ngt2, t1,t2, freq)
tail(ngt2)

filter(ngt2, t1 == "affected")
head(ngt2)
tail(ngt2)
write.csv(ngt2[ngt2$freq>1,],file= paste0(nGramDirectory,"\\N2Grams.csv"),row.names = FALSE)


# prepare ng3 for witing
ngt3<-cbind(ngt3,data.frame(tstrsplit(ngt3[,1],split = " ")))
colnames(ngt3)<-c("word","freq","t1","t2","t3")
ngt3_kn<-ngt3
ngt3$t1<-paste(as.character(ngt3$t1),as.character(ngt3$t2))
ngt3$t2<-ngt2$t3
colnames(ngt3)<-c("word","freq","t1","t2")
ngt3<-select(ngt3, t1,t2,freq)
ngt3 <-  setorder(ngt3, t1, -freq )
filter(ngt3, t1 == "affected by")
head(ngt3)
tail(ngt3)
write.csv(ngt3[ngt3$freq>1,],file= paste0(nGramDirectory,"\\N3Grams.csv"),row.names = FALSE)
```


```{r}
#compute KN smoothing for unigrams based on bigrams
ngt2<-setorder(ngt2,t2,t1)

c1<-data.frame(table(ngt2$t2))
colnames(c1)<-c("word","freq")
c1<-setorder(c1,-freq)
c1<-c1[c1$freq>1,]

head(c1)
tail(c1)
write.csv(c1,file= paste0(nGramDirectory,"\\N1GramsKN.csv"),row.names = FALSE)

#compute KN smoothing for bigrams based on trigrams
ngt3<-ngt3_kn
rm(ngt3_kn)

ngt3$t2<-paste(as.character(ngt3$t2),as.character(ngt3$t3))
colnames(ngt3)<-c("word","freq","t1","t2")
ngt3<-select(ngt3, t1,t2,freq)
ngt3 <-  setorder(ngt3, t2, -freq )

head(ngt3[10000:11000,],50)
tail(ngt3)

c2<-data.frame(table(ngt3$t2))
colnames(c2)<-c("word","freq")
c2<-setorder(c2,-freq)
c2<-c2[c2$freq>1,]

#prepare for writing (like bigrams...)
c2<-c2[c2$freq>1,]
c2<-cbind(c2,data.frame(tstrsplit(c2[,1],split = " ")))
colnames(c2)<-c("word","freq","t1","t2")
c2<-setorder(c2, t1, -freq )
c2<-select(c2, t1,t2, freq)

head(c2)
tail(c2)
write.csv(c2,file= paste0(nGramDirectory,"\\N2GramsKN.csv"),row.names = FALSE)

ff2<-data.frame(table(c2$t1))
ff2<-setorder(ff2, -Freq )
head(ff2)
```


```{r}
# options(mc.cores=4)
# #ngt2[ngt2[,1]=="after",][1:10,]
# library(plyr)
# ?ddply
# dim(c2)
# 
# x<-c2
# head(x)
# x$xs<-0
# ux<-as.character(unique(x$t1))
# for(i in 1:length(ux)){
#     s<-ux[i]
#     s1<-x[x[,1]==s,1]
#     x[x[,1]==s,]$xs<-seq_along(s1)
# }
# 
# 
# head(x,n=100)
# tail(x,n=100)
# 
# x1<-filter(x,xs<10)
# dim(x1)
# head(x1,n=100)
# tail(x1,n=100)
# 
# 
# # If you want to split a dataframe according to values of some variable, I'd suggest using daply() from the plyr package.
# # library(plyr)
# # x <- daply(df, .(splitting_variable), function(x)return(x))
# # 
# # Now, x is an array of dataframes. To access one of the dataframes, you can index it with the name of the level of the splitting variable.
# # x$Level1
# # #or
# # x[["Level1"]]
# # 
# # I'd be sure that there aren't other more clever ways to deal with your data before splitting it up into many dataframes though.
# 
# library(plyr)
# x2 <- daply(c2, .(t1), function(x){(x[,1])})
#  x2$Level1
# #or
# x[["Level1"]]
# x2 <- ddply(c2, .(t1), mutate, xs = seq_along(t1))
```

