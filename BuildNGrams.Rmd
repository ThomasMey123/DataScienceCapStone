---
title: "DataScienceCapstone"
author: "Thomas Mey"
date: "17.3.2016"
output: html_document
---




## setup environment
```{r}
directory<-paste0(".\\data\\", processing.size, "\\en_US")
nGramDirectory<-paste0(".\\data\\", processing.size, "\\NGrams")
if(!file.exists(nGramDirectory)) {
     dir.create(nGramDirectory)
}
```

## read profanity
```{r }
con <- file(".\\data\\profanity_en.txt", "r") 
p<-readLines(con) 
close(con)
``` 


## print summary statistics
```{r}
#Define functions for counting lines, character and words
lineCount<-function(x) {length(x$content)}
charCount<-function(x) {sum(nchar(x$content))}
wordCount<-function(x) {
    sum(sapply(gregexpr("([[:alpha:]])*", x$content), function(x) sum(x > 0)))
}
performCount<-function(d,f) {sapply(1:length(d),function(x){f(d[[x]])})}

#Function for summary as a table
docsSummary<-function(docs) {
  #get line, character and wordcounts
  l2<-performCount(docs,lineCount)
  c2<-performCount(docs,charCount)
  w2<-performCount(docs,wordCount)
  
  
  #Print table with line, character and word counts after cleansing
  rnames<-NULL
  for(i in 1:3){ rnames<-c(rnames,docs[[i]]$meta$id)}
  d<-data.frame(l2,w2,c2)
  rownames(d)<-rnames
  colnames(d)<-c("#Lines","#Words","#Characters")
  suppressWarnings( require(htmlTable))
  htmlTable(d)
}
```

#preprocess and build dtm
```{r}
preprocess<-function(docs,p) {
  msg<-Sys.setlocale("LC_ALL","English")
  
  #Remove URLS
  removeURL<-function(x) gsub("http[^[:space:]]*","",x)
  docs <- tm_map(docs, content_transformer(removeURL))   
  
  # Handle right single quotes
  # http://stackoverflow.com/questions/2477452/%C3%A2%E2%82%AC%E2%84%A2-showing-on-page-instead-of
  
  #replaceQuotes<-function(x) gsub("(\ue2\u80\u99\ub4\u92)", "'", x)
  replaceQuotes<-function(x) gsub("(\u92\ub4)", "'", x,perl=TRUE)
  
  docs <- tm_map(docs, content_transformer(replaceQuotes))
  #replaceQuotes2<-function(x) gsub("(\u02BC\u02C8\u0301\u05F3\u2032\uA78C\u02B9)", "'", x, perl = TRUE)
  #docs <- tm_map(docs, content_transformer(replaceQuotes2))
  
  #remove anything but english letters or space
  removeNumPunct<-function(x) gsub("[^A-Za-z']"," ",x)
  docs <- tm_map(docs, content_transformer(removeNumPunct))   

  #tolower
  docs <- tm_map(docs, content_transformer(tolower))

  #remove any character sequence where a character is repeated more than 4 times  
  removeRepeatedChars<-function(x) gsub("(.)\\1{3,}","",x)
  docs <- tm_map(docs, content_transformer(removeRepeatedChars))   

  #remove profanity 
  docs <- tm_map(docs, removeWords, p)
  

  #strip whitespace
  docs <- tm_map(docs, stripWhitespace) 
  docs
}

library(tm)   
docs <- Corpus(DirSource(directory))
docsSummary(docs)

docs<-preprocess(docs,p)

docs[[3]]$content[500]

corpus_df <- data.frame(text=lapply(docs[1], '[',"content"), stringsAsFactors=F)
corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[2], '[',"content"), stringsAsFactors=F))
corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[3], '[',"content"), stringsAsFactors=F))


corpus_df<-corpus_df$content
corpus_df[19]

corpus_df<-paste(corpus_df,collapse = "")

require(quanteda)

ng1<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=1)
ng2<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=2)
ng3<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=3)

require(data.table)
nGramTable <- function( nGramTokens) {
  nGramTable <- data.frame(table(nGramTokens))
  colnames(nGramTable) <- c("word", "freq")
  
  nGramTable <-  nGramTable[order(nGramTable$freq,decreasing=TRUE),]
 
  nGramTable
}


ngt1<-nGramTable(ng1)
rownames(ngt1)<-NULL
ngt1<-ngt1[ngt1$freq>1,]
head(ngt1)
tail(ngt1)
write.csv(ngt1,file= paste0(nGramDirectory,"\\N1Grams.csv"),row.names = FALSE)


ngt2<-nGramTable(ng2)
rownames(ngt2)<-NULL
ngt2<-ngt2[ngt2$freq>1,]
head(ngt2)
tail(ngt2)
write.csv(ngt2,file= paste0(nGramDirectory,"\\N2Grams.csv"),row.names = FALSE)

ngt3<-nGramTable(ng3)
rownames(ngt3)<-NULL
ngt3<-ngt3[ngt3$freq>1,]
head(ngt3)
tail(ngt3)
write.csv(ngt3,file= paste0(nGramDirectory,"\\N3Grams.csv"),row.names = FALSE)
```




