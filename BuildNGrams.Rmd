---
title: "DataScienceCapstone"
author: "Thomas Mey"
date: "17.3.2016"
output: html_document
---




## setup environment
```{r}
library(data.table)
library(dplyr)
library(tm) 
require(quanteda)


source("commonProcess.R")
source("WordPredictionApp/Common.r")

```

## read profanity
```{r }
p<-readProfanity()
``` 


## print summary statistics
```{r}
#Define functions for counting lines, character and words
lineCount<-function(x) {length(x$content)}
charCount<-function(x) {sum(nchar(x$content))}
wordCount<-function(x) {
    sum(sapply(gregexpr("([[:alpha:]])*", x$content), function(x) sum(x > 0)))
}
performCount<-function(d,f) {sapply(1:length(d),function(x){f(d[[x]])})}

#Function for summary as a table
docsSummary<-function(docs) {
  #get line, character and wordcounts
  l2<-performCount(docs,lineCount)
  c2<-performCount(docs,charCount)
  w2<-performCount(docs,wordCount)
  
  
  #Print table with line, character and word counts after cleansing
  rnames<-NULL
  for(i in 1:3){ rnames<-c(rnames,docs[[i]]$meta$id)}
  d<-data.frame(l2,w2,c2)
  rownames(d)<-rnames
  colnames(d)<-c("#Lines","#Words","#Characters")
  suppressWarnings( require(htmlTable))
  htmlTable(d)
}
```

#preprocess and build dtm
```{r}
options(mc.cores=1)

for(i in 8:length(percentages)){
  percentage<-percentages[i]
  print(paste0("Building ngrams with ", percentage, "%"  ))

  trainDir<-makeFileName(dir.root,percentage,dir.train)
  nGramDirectory<-makeFileName(dir.root,percentage,dir.ngrams)
  createDir(nGramDirectory)
  nGramDirectory<-substr(nGramDirectory,3,10000L)
  
  docs <- Corpus(DirSource(trainDir))
  docsSummary(docs)
  
  docs<-preprocess(docs,p)
  
  docs[[3]]$content[500]
  
  corpus_df <- data.frame(text=lapply(docs[1], '[',"content"), stringsAsFactors=F)
  corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[2], '[',"content"), stringsAsFactors=F))
  corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[3], '[',"content"), stringsAsFactors=F))
  
  
  corpus_df<-corpus_df$content
  corpus_df[19]
  
  corpus_df<-paste(corpus_df,collapse = "")
  
 
  ng1<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=1)
  ng2<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=2)
  ng3<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=3)
  #ng4<-tokenize(corpus_df, what="fastestword",concatenator = " ", ngrams=4)
  
  
  # Build NGram Tables
  ngt1<-nGramTable(ng1)
  rownames(ngt1)<-NULL
  
  ngt2<-nGramTable(ng2)
  rownames(ngt2)<-NULL
  
  ngt3<-nGramTable(ng3)
  rownames(ngt3)<-NULL
  
  #ngt4<-nGramTable(ng4)
  #rownames(ngt4)<-NULL

  rm(corpus_df,ng1,ng2,ng3)
  
  #Prepare ng1 for writing
  ngt1<-setorder(ngt1, -freq )
  write.csv(ngt1,file= makeFileName(nGramDirectory, ngramFile.1F),row.names = FALSE)
  write.csv(ngt1[ngt1$freq>1,],file= makeFileName(nGramDirectory, ngramFile.1),row.names = FALSE)
  
  
  # prepare ng2 for witing
  ngt2<-cbind(ngt2,data.frame(tstrsplit(ngt2[,1],split = " ")))
  colnames(ngt2)<-c("word","freq","t1","t2")
  #ngt2$t1<-as.character(ngt2$t1)
  #ngt2$t2<-as.character(ngt2$t2)
  ngt2<-setorder(ngt2, t1, -freq )
  ngt2<-select(ngt2, t1,t2, freq)
  tail(ngt2)
  
  filter(ngt2, t1 == "affected")
  head(ngt2)
  tail(ngt2)
  write.csv(ngt2,file= makeFileName(nGramDirectory, ngramFile.2F),row.names = FALSE)
  write.csv(ngt2[ngt2$freq>1,],file= makeFileName(nGramDirectory, ngramFile.2),row.names = FALSE)
  
  
  # prepare ng3 for witing
  ngt3<-cbind(ngt3,data.frame(tstrsplit(ngt3[,1],split = " ")))
  colnames(ngt3)<-c("word","freq","t1","t2","t3")
  ngt3$t1<-paste(as.character(ngt3$t1),as.character(ngt3$t2))
  ngt3$t2<-ngt2$t3
  colnames(ngt3)<-c("word","freq","t1","t2")
  ngt3<-select(ngt3, t1,t2,freq)
  ngt3 <-  setorder(ngt3, t1, -freq )
  filter(ngt3, t1 == "affected by")
  head(ngt3)
  tail(ngt3)
  write.csv(ngt3,file= makeFileName(nGramDirectory, ngramFile.3F),row.names = FALSE)
  write.csv(ngt3[ngt3$freq>1,],file= makeFileName(nGramDirectory, ngramFile.3),row.names = FALSE)

}

```



```{r}
# rm(ngt2,ngt3,ngt1,docs)
# gc()
# library(plyr)
# 
# c2<-c2[100000:200000,]
# c2$index<-seq_along(c2$t1)
# x3<-ddply(c2, .(t1), mutate, xs = index -min(index) +1)
# 
# head(x3,n=100)
# tail(x3,n=100)
# 
# x1<-filter(x3,xs<10)
# dim(x1)
# head(x1,n=100)
# tail(x1,n=100)
```


