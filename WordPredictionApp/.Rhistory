?read
??read
?readLines
unzip(zipfile=".\\data\\Coursera-SwiftKey.zip", exdir=".\\data")
n1<-paste0(".\\data\\final\\en_US\\",name)
n2<-paste0(".\\data\\small\\en_US\\",name)
con1<-file(n1,"r")
t1<-readLines(con1)
close(con1)
l1<-length(t1)
l1
n1<-paste0(".\\data\\final\\en_US\\",name)
n2<-paste0(".\\data\\small\\en_US\\",name)
con1<-file(n1,"r")
t1<-readLines(con1,encoding = "UTF8")
close(con1)
l1<-length(t1)
t2<-sample(t1,l1/100*percentage)
con2<-file(n2,"w")
l1
t1<-readLines(con1,encoding = "UTF8",warn=FALSE)
con1<-file(n1,"r")
t1<-readLines(con1,encoding = "UTF8",warn=FALSE)
close(con1)
l1<-length(t1)
l1
?file
con1<-file(n1,"r",blocking = FALSE )
t1<-readLines(con1,encoding = "UTF8")
close(con1)
l1<-length(t1)
l1
con1<-file(n1,"rb" )
t1<-readLines(con1,encoding = "UTF8")
close(con1)
l1<-length(t1)
l1
t2<-sample(t1,l1/100*percentage)
con2<-file(n2,"w")
writeLines(t2,con2)
close(con2)
nGramDirectory<-paste0(".\\data\\", processing, "\\NGrams")
if(!file.exists(nGramDirectory)) {
dir.create(nGramDirectory)
}
if(processing == "final") {
processing<-"small"
directory<-paste0(".\\data\\", processing, "\\en_US")
docs <- Corpus(DirSource(directory))
docs <- preprocess(docs)
}
dtm <- DocumentTermMatrix(docs, control = list(wordLengths = c(1,Inf)))
#rowSums(as.matrix(dtm))
#could use the below findFreqTerms, but the code below gives more detailed output
#findFreqTerms(dtm,lowfreq = 2000)
#The most frequent terms
freq <- colSums(as.matrix(dtm  ))
allwords<-sum(freq)
ord <- order(freq,decreasing = TRUE)
orderedFreqs<-data.frame(freq[ord])
colnames(orderedFreqs)<-c("Occurances")
orderedFreqs$percentage<-orderedFreqs$Occurances/allwords*100
#head(orderedFreqs,n=50)
htmlTable(head(orderedFreqs,n=30))
par(mfrow=c(1,2))
#A histogram with the distribution of terms
#hist(freq, breaks = 5000)
#with logarithmic scale
r <- hist(freq, breaks = 5000)
plot(r$breaks[-1], r$counts, log='xy', type='h', xlab="freq (log)", ylab="Frequency(log)")
source('~/.active-rstudio-document', echo=TRUE)
makeSmall("en_US.twitter.txt",1)
makeSmall("en_US.blogs.txt",1)
makeSmall("en_US.news.txt",1)
name<-"en_US.blogs.txt"
n1<-paste0(".\\data\\final\\en_US\\",name)
con1<-file(n1,"rb" )
t1<-readLines(con1,encoding = "UTF8")
close(con1)
grep("The guy in front of me just bought a pound of bacon, a bouquet, and a case of",t1)
processing<-"final"
directory<-paste0(".\\data\\", processing, "\\en_US")
name1<-"en_US.twitter.txt"
name2<-"en_US.blogs.txt"
name3<-"en_US.news.txt"
n1<-paste0(".\\data\\final\\en_US\\",name1)
con1<-file(n1,"rb" )
t1<-readLines(con1,encoding = "UTF8")
close(con1)
n2<-paste0(".\\data\\final\\en_US\\",name2)
con2<-file(n2,"rb" )
t2<-readLines(con2,encoding = "UTF8")
close(con2)
n3<-paste0(".\\data\\final\\en_US\\",name3)
con3<-file(n3,"rb" )
t3<-readLines(con3,encoding = "UTF8")
close(con3)
grep("The guy in front of me just bought a pound of bacon, a bouquet, and a case of",t1)
grep("The guy in front of me just bought a pound of bacon, a bouquet, and a case of",t2)
grep("The guy in front of me just bought a pound of bacon, a bouquet, and a case of",t3)
grep("pound of bacon",t1)
matches1<-grep("pound of bacon",t1)
t1(matches)
t1[matches1]
matches2<-grep("pound of bacon",t2)
t2[matches2]
matches3<-grep("pound of bacon",t3)
t3[matches3]
matches1<-grep("a bouquet",t1)
matches2<-grep("a bouquet",t2)
matches3<-grep("a bouquet",t3)
t1[matches1]
t3[matches3]
nGram
nGramDirectory<-paste0(".\\data\\", processing, "\\NGrams")
n1GramTable<-read.csv(paste0(nGramDirectory,"\\N1Grams.csv"))
processing<-"small"
nGramDirectory<-paste0(".\\data\\", processing, "\\NGrams")
n1GramTable<-read.csv(paste0(nGramDirectory,"\\N1Grams.csv"))
n2GramTable<-read.csv(paste0(nGramDirectory,"\\N2Grams.csv"))
n3GramTable<-read.csv(paste0(nGramDirectory,"\\N3Grams.csv"))
n4GramTable<-read.csv(paste0(nGramDirectory,"\\N4Grams.csv"))
getTail <-function(x,nWords=3){
matches<-gregexpr("[[:alpha:](\'[:alpha:])?]+",x)
tail(matches[[1]],n=nWords)
m3<-tail(matches[[1]],n=nWords)
res<-substr(x,m3[1],nchar(x))
trimws(res)
}
makeNGramMatchRegex<-function(x){
paste0("^",x," ")
}
predictWord <-function(test) {
print(paste0("Predicting for \'",test,"\'"))
testNGram<-getTail(test)
testNGram<-makeNGramMatchRegex(testNGram)
f<-grep(testNGram,n4GramTable$word)
if(length(f)>0){
print("Recommendations based on three words")
n4GramTable[f[1:3],]
} else {
testNGram2<-getTail(testNGram,2)
testNGram2<-makeNGramMatchRegex(testNGram2)
f<-grep(testNGram2,n3GramTable$word)
if(length(f)>0){
print("Recommendations based on two words")
n3GramTable[f[1:3],]
} else {
testNGram1<-getTail(testNGram,1)
testNGram1<-makeNGramMatchRegex(testNGram1)
f<-grep(testNGram1,n2GramTable$word)
if(length(f)>0){
print("Recommendations based on two words")
n2GramTable[f[1:3],]
} else{
print("No recommendation found")
}
}
}
}
predictWord("The guy in front of me just bought a pound of bacon, a bouquet, and a case of")
predictWord("You're the reason why I smile everyday. Can you follow me please? It would mean the")
predictWord("Hey sunshine, can you follow me and make me the")
predictWord("Very early observations on the Bills game: Offense still struggling but the")
predictWord("Go on a romantic date at the")
predictWord("Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my")
predictWord("Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some")
predictWord("After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little")
predictWord("Be grateful for the good times and keep the faith during the")
predictWord("faith during the hard")
predictWord("faith during the hard")
predictWord("faith during the sad")
predictWord("faith during the worse")
predictWord("faith during the bad")
predictWord("faith during the")
predictWord("If this isn't the cutest thing you've ever seen, then you must be")
#processing<-"final"
processing<-"small"
#processing<-"test"
makeSmall<-function(name,percentage){
n1<-paste0(".\\data\\final\\en_US\\",name)
n2<-paste0(".\\data\\small\\en_US\\",name)
con1<-file(n1,"rb" )
t1<-readLines(con1,encoding = "UTF8")
close(con1)
l1<-length(t1)
t2<-sample(t1,l1/100*percentage)
con2<-file(n2,"w")
writeLines(t2,con2)
close(con2)
}
set.seed(123)
predictNoisyWord <-function(test) {
print(paste0("Predicting for \'",test,"\'"))
testNGram<-getTail(test)
testNGram<-makeNGramMatchRegex(testNGram)
f<-grep(testNGram,n4GramTable$word)
if(length(f)>0){
print("Recommendations based on three words")
n4GramTable[f[1:3],]
} else {
testNGram2<-getTail(testNGram,2)
testNGram2<-makeNGramMatchRegex(testNGram2)
f<-grep(testNGram2,n3GramTable$word)
if(length(f)>0){
print("Recommendations based on two words")
n3GramTable[f[1:3],]
} else {
testNGram1<-getTail(testNGram,1)
testNGram1<-makeNGramMatchRegex(testNGram1)
f<-grep(testNGram1,n2GramTable$word)
if(length(f)>0){
print("Recommendations based on two words")
n2GramTable[f[1:3],]
} else{
print("No recommendation found")
}
}
}
}
DirSource
library(tm)
?DirSource
?VectorSource
x<-"This is a sample dataset with implicit meaning"
tailDoc <- Corpus(VectorSource(x))
tailDoc[[1]]
preprocess<-function(docs) {
msg<-Sys.setlocale("LC_ALL","English")
#preprocess
removeURL<-function(x) gsub("http[^[:space:]]*","",x)
docs <- tm_map(docs, content_transformer(removeURL))
#remove anything but english letters or space
removeNumPunct<-function(x) gsub("[^[:alpha:][:space:]]*","",x)
docs <- tm_map(docs, content_transformer(removeNumPunct))
#remove profanity and whitespace
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, p)
docs <- tm_map(docs, removeWords, c(stopwords('english')))
docs <- tm_map(docs, stripWhitespace)
docs
}
tailDoc <- Corpus(VectorSource(x))
tailDoc<-preprocess(tailDoc)
con <- file(".\\data\\profanity_en.txt", "r")
p<-readLines(con)
close(con)
preprocess<-function(docs,p) {
msg<-Sys.setlocale("LC_ALL","English")
#preprocess
removeURL<-function(x) gsub("http[^[:space:]]*","",x)
docs <- tm_map(docs, content_transformer(removeURL))
#remove anything but english letters or space
removeNumPunct<-function(x) gsub("[^[:alpha:][:space:]]*","",x)
docs <- tm_map(docs, content_transformer(removeNumPunct))
#remove profanity and whitespace
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, p)
docs <- tm_map(docs, removeWords, c(stopwords('english')))
docs <- tm_map(docs, stripWhitespace)
docs
}
docs<-preprocess(docs,p)
tailDoc <- Corpus(VectorSource(x))
tailDoc<-preprocess(tailDoc,p)
tailDoc[[1]]
tailDoc[[1]]$content
getTail(tailDoc[[1]]$content)
getTail <-function(x,nWords=3){
matches<-gregexpr("[[:alpha:](\'[:alpha:])?]+",x)
tail(matches[[1]],n=nWords)
m3<-tail(matches[[1]],n=nWords)
res<-substr(x,m3[1],nchar(x))
trimws(res)
}
makeNGramMatchRegex<-function(x){
paste0("^",x," ")
}
getTail(tailDoc[[1]]$content)
n1GramTable<-read.csv(paste0(nGramDirectory,"\\N1Grams.csv"))
n2GramTable<-read.csv(paste0(nGramDirectory,"\\N2Grams.csv"))
processing<-"small"
directory<-paste0(".\\data\\", processing, "\\en_US")
nGramDirectory<-paste0(".\\data\\", processing, "\\NGrams")
if(!file.exists(nGramDirectory)) {
dir.create(nGramDirectory)
}
n1GramTable<-read.csv(paste0(nGramDirectory,"\\N1Grams.csv"))
n2GramTable<-read.csv(paste0(nGramDirectory,"\\N2Grams.csv"))
n3GramTable<-read.csv(paste0(nGramDirectory,"\\N3Grams.csv"))
n4GramTable<-read.csv(paste0(nGramDirectory,"\\N4Grams.csv"))
predictWord <-function(test) {
print(paste0("Predicting for \'",test,"\'"))
testNGram<-getCleanTail(test)
testNGram<-makeNGramMatchRegex(testNGram)
f<-grep(testNGram,n4GramTable$word)
if(length(f)>0){
print("Recommendations based on three words")
n4GramTable[f[1:3],]
} else {
testNGram2<-getTail(testNGram,2)
testNGram2<-makeNGramMatchRegex(testNGram2)
f<-grep(testNGram2,n3GramTable$word)
if(length(f)>0){
print("Recommendations based on two words")
n3GramTable[f[1:3],]
} else {
testNGram1<-getTail(testNGram,1)
testNGram1<-makeNGramMatchRegex(testNGram1)
f<-grep(testNGram1,n2GramTable$word)
if(length(f)>0){
print("Recommendations based on two words")
n2GramTable[f[1:3],]
} else{
print("No recommendation found")
}
}
}
}
predictWord("You should have gone")
getCleanTail <-function(x,nWords=3){
tailDoc <- Corpus(VectorSource(x))
tailDoc<-preprocess(tailDoc,p)
getTail(tailDoc[[1]]$content)
}
predictWord("You should have gone")
predictWord("Don't try to be a")
predictWord("The guy in front of me just bought a pound of bacon, a bouquet, and a case of")
predictWord("You're the reason why I smile everyday. Can you follow me please? It would mean the")
predictWord("Hey sunshine, can you follow me and make me the")
predictWord("Go on a romantic date at the")
predictWord("Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my")
predictWord("Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some")
predictWord("After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little")
predictWord("faith during the")
predictWord("If this isn't the cutest thing you've ever seen, then you must be")
makeSmall<-function(name,percentage){
n1<-paste0(".\\data\\final\\en_US\\",name)
n2<-paste0(".\\data\\small\\en_US\\",name)
con1<-file(n1,"rb" )
t1<-readLines(con1,encoding = "UTF8")
close(con1)
l1<-length(t1)
t2<-sample(t1,l1/100*percentage)
con2<-file(n2,"w")
writeLines(t2,con2)
close(con2)
}
makeSmall("en_US.twitter.txt",2)
makeSmall("en_US.blogs.txt",2)
makeSmall("en_US.news.txt",2)
processing.size<-"small"
n1GramTable<-read.csv(paste0(nGramDirectory,"\\N1Grams_",processing.mode, ".csv"))
processing.mode<-"clean"
n1GramTable<-read.csv(paste0(nGramDirectory,"\\N1Grams_",processing.mode, ".csv"))
n2GramTable<-read.csv(paste0(nGramDirectory,"\\N2Grams_",processing.mode, ".csv"))
n3GramTable<-read.csv(paste0(nGramDirectory,"\\N3Grams_",processing.mode, ".csv"))
n4GramTable<-read.csv(paste0(nGramDirectory,"\\N4Grams_",processing.mode, ".csv"))
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
setwd("~/git-repos/DataScienceCapStone")
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
rsconnect::showLogs()
?rsconnect::showLogs()
install.packages('rsconnect')
install.packages("rsconnect")
library(rsconnect)
?rsconnect::showLogs()
rsconnect::showLogs()
rsconnect::showLogs(".")
getwd()
rsconnect::showLogs("WordPrdictionApp")
source("Common.r")
?tolower
setwd("~/git-repos/DataScienceCapStone")
source("Common.r")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
length(docs[[3]])
length(docs[[3]]$content)
length(docs[[1]]$content)
length(docs[[2]]$content)
length(docs[[3]]$content)
corpus_df <- data.frame(text=lapply(docs[1], '[',"content"), stringsAsFactors=F)
corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[2], '[',"content"), stringsAsFactors=F))
corpus_df <- rbind(corpus_df,data.frame(text=lapply(docs[3], '[',"content"), stringsAsFactors=F))
length(corpus_df)
corpus_df<-corpus_df$content
corpus_df[19]
length(corpus_df)
n<-sample(length(corpus_df),100)
n
n<-sample(length(corpus_df),10)
n
corpus_df[n]
testset<-corpus_df[n]
testset<-corpus_df[n]
testset
source('~/.active-rstudio-document', echo=TRUE)
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
print(getwd())
setwd("C:/Dev/Repos/10-DataScienceCapStone/WordPredictionApp")
source("Common.R")
setwd("C:/Dev/Repos/10-DataScienceCapStone/WordPredictionApp")
setwd("C:\\Dev\\Repos\\10-DataScienceCapStone\\WordPredictionApp")
setwd("~/git-repos/DataScienceCapStone/WordPredictionApp")
source("Common.R")
n1GramTable<-read.csv("Data/N1GramsKN.csv",colClasses = c("character","integer"))
n2GramTable<-read.csv("Data/N2GramsKN.csv",colClasses = c("character","character","integer"))
n3GramTable<-read.csv("Data/N3Grams.csv",colClasses = c("character","character","integer"))
n3GramTable<-read.csv("Data/N3Grams.csv",colClasses = c("character","character","integer"))
head(n1GramTable)
head(n2GramTable)
head(n3GramTable)
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
test<-"I"
x<-"I"
nWords<-3
matches<-gregexpr("[[:alpha:](\'[:alpha:])?]+",x)
matches
m3<-tail(matches[[1]],n=nWords)
m3
matches[[1]]
tail(matches[[1]],n=nWords)
res<-substr(x,m3[1],nchar(x))
res
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
x<-""
matches<-gregexpr("[[:alpha:](\'[:alpha:])?]+",x)
matches
m3<-tail(matches[[1]],n=nWords)
m3
res<-substr(x,m3[1],nchar(x))
res
substr(x,m3[1],nchar(x))
x<-NULL
matches<-gregexpr("[[:alpha:](\'[:alpha:])?]+",x)
m3<-tail(matches[[1]],n=nWords)
matches[[1]]
matches
length(matches)
if(x==NULL) x<-""
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
predictWord("This is my favorite holiday")
predictWord("This is my favorite holiday",5)
source("Common.R")
predictWord <-function(test,n) {
if(debug.print){
print(paste0("Predicting for \'",test,"\'"))
}
testNGram<-getTail(test)
testNGram2<-getTail(testNGram,2)
f<-filter(n3GramTable, t1 == testNGram2)
r<-NULL
if(nrow(f)>0){
r<-f[1:(min(nrow(f),n)),2]
}
if(length(r)<n)
{
testNGram1<-getTail(testNGram,1)
f<-filter(n2GramTable, t1 == testNGram1)
if(nrow(f)>0){
r<-c(r,f[1:min(nrow(f),n-length(r)),2])
}
}
if(length(r)<n)
{
r2<-n1GramTable[1:(n-length(r)),1]
r<-c(r,r2)
}
r
}
predictWord("This is my favorite holiday",5)
setwd("~/git-repos/DataScienceCapStone")
library(shiny)
runApp("WordPredictionApp")
shiny::runApp('WTest2')
shiny::runApp('WTest2')
shiny::runApp('WTest2')
shiny::runApp('WTest2')
shiny::runApp('WTest2')
shiny::runApp('WTest2')
shiny::runApp()
shiny::runApp()
shiny::runApp()
?tolower
shiny::runApp()
